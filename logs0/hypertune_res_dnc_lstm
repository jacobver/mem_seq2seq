 data: data/multi30k.atok.low.train.pt
 data: data/multi30k.atok.low.train.pt
 data: data/multi30k.atok.low.train.pt
 =====  better result ====

low ppl: 30.177234 
 number of params: 23323498 
 epoch: 11
 train ppls: [455.2195329601432, 70.46083886874717, 37.240671573621235, 27.56945890556923, 21.92931160351366, 18.227795596873683, 17.129227302808705, 14.859700150425736, 13.205003551108629, 11.883674247586292, 10.68153579190593, 9.695860710797533, 8.811228184534151, 8.10223911930931] 
 vaild ppls: [223.17273478736993, 59.074163766757245, 45.98022876417666, 42.753606871998, 40.681834685179176, 39.34435752332701, 30.22253446898153, 29.8261294434588, 29.741340294270344, 29.882318171001053, 29.706750257534303, 29.755419067527054, 30.177233901680683, 31.5829438118733] 

Namespace(attn=0.0, batch_size=64, brnn=1, brnn_merge='concat', cat_mo_special=False, context_sz=1, curriculum=6, data='data/multi30k.atok.low.train.pt', dropout=0.13, encoder_type='text', epochs=79, extra_shuffle=False, gather_net_data=0, gpus=[0], hops=8, in_size=500, input_feed=2.0, layers=2, learning_rate=0.004757, learning_rate_decay=0.73, linear_start=1, log_interval=50, max_generator_batches=32, max_grad_norm=5, mem='dnc_lstm', mem_size=139, mem_slots=26, n_samples=10, optim='adam', out_size=500, param_init=0.1, pre_word_vecs_dec=None, pre_word_vecs_enc='data/multi30k.atok.low.src.emb.pt', prev_opts=None, read_heads=1, rnn_size=300, save_model='models/model', seq='decoder', share_M=0, start_decay_at=12, start_epoch=1, train_from='', train_from_state_dict='', word_vec_size=300)

===========================================================


 =====  better result ====

low ppl: 13.263346 
 number of params: 32631198 
 epoch: 12
 train ppls: [86.00172223838328, 37.73334480640729, 25.1199760207354, 19.0719493922361, 15.411729011955352, 13.075086642199727, 11.369760858028219, 10.041733600358237, 9.16779862149029, 8.251660970666768, 7.588222319286033, 6.970855322029144, 6.502464839863523, 6.064405467746559, 5.711981090204486] 
 vaild ppls: [53.771265241858, 30.230118548161997, 22.818389880989614, 19.457335570149247, 16.497328259482117, 15.296627348933148, 14.419016255419773, 13.776835426826532, 12.84604383495824, 12.799639881607158, 12.669014386251641, 12.601551300531861, 12.917046042871576, 13.263346089611133, 13.364972342961416] 

Namespace(attn=1.0, batch_size=64, brnn=0, brnn_merge='concat', cat_mo_special=False, context_sz=1, curriculum=8, data='data/multi30k.atok.low.train.pt', dropout=0.69, encoder_type='text', epochs=79, extra_shuffle=False, gather_net_data=0, gpus=[0], hops=8, in_size=500, input_feed=2.0, layers=2, learning_rate=0.003605, learning_rate_decay=0.53, linear_start=1, log_interval=50, max_generator_batches=32, max_grad_norm=5, mem='dnc_lstm', mem_size=113, mem_slots=11, n_samples=10, optim='adam', out_size=500, param_init=0.1, pre_word_vecs_dec=None, pre_word_vecs_enc=None, prev_opts=None, read_heads=2, rnn_size=400, save_model='models/model', seq='decoder', share_M=1, start_decay_at=22, start_epoch=1, train_from='', train_from_state_dict='', word_vec_size=400)

===========================================================


