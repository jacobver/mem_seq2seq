 data: data/multi30k.atok.low.train.pt
 =====  better result ====

low ppl: 9.177545 
 number of params: 23216398 
 epoch: 8
 train ppls: [66.28909068678823, 25.410497851985532, 15.502644510103954, 10.0156093130437, 7.1846614265932125, 5.587299485437065, 4.612471197996112, 3.913695005073921, 3.439410898095087, 3.0988395077788073, 2.8524722969949927] 
 vaild ppls: [44.56554162021496, 25.53071534095586, 13.963337407371625, 11.290777443992697, 9.726312022061467, 9.038639520097195, 8.90492325801504, 8.751556381493467, 8.965721176247104, 9.177544690905396, 9.181179511082037] 

Namespace(attn=0.0, batch_size=64, brnn=1, brnn_merge='concat', cat_mo_special=False, context_sz=1, curriculum=2, data='data/multi30k.atok.low.train.pt', dropout=0.36, encoder_type='text', epochs=79, extra_shuffle=False, gather_net_data=0, gpus=[0], hops=8, in_size=500, input_feed=1.0, layers=2, learning_rate=0.002735, learning_rate_decay=0.49, linear_start=1, log_interval=50, max_generator_batches=32, max_grad_norm=5, mem='nse_lstm', mem_size=100, mem_slots=20, n_samples=10, optim='adam', out_size=500, param_init=0.1, pre_word_vecs_dec=None, pre_word_vecs_enc='data/multi30k.atok.low.src.emb.pt', prev_opts=None, read_heads=1, rnn_size=300, save_model='models/model', seq='decoder', share_M=1, start_decay_at=9, start_epoch=1, train_from='', train_from_state_dict='', word_vec_size=300)

===========================================================


low ppl: 61.634756 
 number of params: 22495798 
 epoch: 31
 train ppls: [1353.1068236770789, 360.7632113884156, 264.0723454959486, 225.49426082770674, 202.12163885743433, 175.0818004263863, 151.51276444415024, 135.45469473522806, 123.31262497809081, 113.3263202097044, 105.2931108186714, 98.35975904314844, 91.95554437757853, 86.82008147438951, 82.0336400688388, 77.96257076741556, 74.3897649623868, 71.29188051887816, 68.5282956605319, 66.6315490506549, 65.37518330706556, 64.64469997165074, 64.30544457684485, 63.77131530617924, 63.55425670389666, 63.2895511990231, 63.24069424671253, 63.139133514981744, 63.05056128470179, 63.06849424768053, 63.120791649998075, 62.99580527031267, 62.944554320475554, 62.99784435800508] 
 vaild ppls: [408.1762102379671, 270.36857574427336, 195.59634985288324, 181.0265891071969, 165.05026972703567, 141.30425923228412, 126.61109496288277, 113.67136914549712, 104.86036239448882, 96.93950529984255, 90.44688308592924, 85.0855977698487, 80.65122329079881, 76.34868028995304, 72.9479849847264, 70.78953235929505, 67.6375078682441, 65.98099622814972, 63.96951252044825, 63.21411388202807, 62.44860711566501, 62.533494772053224, 62.08933805013105, 61.80562623748622, 61.74959759053374, 61.743190511712186, 61.673562804785234, 61.70669994532278, 61.689268515713366, 61.67686093161446, 61.61467013616913, 61.63183037449818, 61.63475628387122, 61.6408349677996] 

Namespace(attn=0.0, batch_size=64, brnn=0, brnn_merge='concat', cat_mo_special=False, context_sz=1, curriculum=2, data='data/multi30k.atok.low.train.pt', dropout=0.7, encoder_type='text', epochs=79, extra_shuffle=False, gather_net_data=0, gpus=[0], hops=8, in_size=500, input_feed=0.0, layers=2, learning_rate=4.1e-05, learning_rate_decay=0.63, linear_start=1, log_interval=50, max_generator_batches=32, max_grad_norm=5, mem='nse_lstm', mem_size=100, mem_slots=20, n_samples=10, optim='adam', out_size=500, param_init=0.1, pre_word_vecs_dec=None, pre_word_vecs_enc='data/multi30k.atok.low.src.emb.pt', prev_opts=None, read_heads=1, rnn_size=300, save_model='models/model', seq='decoder', share_M=1, start_decay_at=18, start_epoch=1, train_from='', train_from_state_dict='', word_vec_size=300)

===========================================================


